{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75fc7b5-4e8f-4413-8bf3-b004d67cb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d110a52-e8d7-4f0b-89a5-c008ab969172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Book Recommendation\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"50g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36bd01e1-63e3-4186-91d6-e323fc6d3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\abdel\\Downloads\\BigData_Project\\Sales_Data\\Books_rating.csv\"\n",
    "rating_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "columns = ['Id','User_id','review/score','Title','review/text']\n",
    "new_names = ['book_id','user_id','rating','title','review']\n",
    "rating_df = rating_df.select([col for col in columns]).toDF(*new_names)\n",
    "rating_df = rating_df.withColumn(\"rating\", rating_df[\"rating\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a29116-f005-402b-8088-6820980efb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfd8a2b-b37d-4ba1-9c3e-fb29e2dbe677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      "\n",
      "Size of values:  2420256\n",
      "+----------+--------------+------+--------------------+--------------------+\n",
      "|   book_id|       user_id|rating|               title|              review|\n",
      "+----------+--------------+------+--------------------+--------------------+\n",
      "|1882931173| AVCGYZL8FQQTD|   4.0|Its Only Art If I...|This is only for ...|\n",
      "|0826414346|A30TK6U7DNS82R|   5.0|Dr. Seuss: Americ...|I don't care much...|\n",
      "|0826414346|A3UH4UZ4RSVO82|   5.0|Dr. Seuss: Americ...|\"If people become...|\n",
      "|0826414346|A2MVUWT453QH61|   4.0|Dr. Seuss: Americ...|Theodore Seuss Ge...|\n",
      "|0826414346|A22X4XUPKF66MR|   4.0|Dr. Seuss: Americ...|\"Philip Nel - Dr....|\n",
      "+----------+--------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_df.printSchema()\n",
    "print(\"Size of values: \",rating_df.count())\n",
    "rating_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c9366dd-ebb1-4b79-a1be-ebd38565bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+--------------------+--------------------+\n",
      "|   book_id|       user_id|rating|               title|              review|\n",
      "+----------+--------------+------+--------------------+--------------------+\n",
      "|0826414346|A2RSSXTDZDUSH4|   5.0|Dr. Seuss: Americ...|\"When I recieved ...|\n",
      "|0595344550| AUR0VA5H0C66C|   1.0|Whispers of the W...|\"This is a self-p...|\n",
      "|0595344550| ACO23CG8K8T77|   5.0|Whispers of the W...|I read the review...|\n",
      "|0802841899|A2H2LORTA5EZY2|   4.0|The Church of Chr...|This is a very us...|\n",
      "|0854968350| ATDE9JYCPI0L1|   2.0|Muslim Women's Ch...|\"I was excited to...|\n",
      "|0918973031|A32ZQ5DEXBL60Z|   5.0|Dramatica for Scr...|\"I think the hard...|\n",
      "|0918973031|A1X1CW1GXKC50V|   5.0|Dramatica for Scr...|This is the way t...|\n",
      "|0792391810| A29Z0B2L367ZO|   5.0|Vector Quantizati...|It seems somebody...|\n",
      "|0974289108|A3AJA5ADM3Q8LM|   5.0|\"The Ultimate Gui...|Boy am I lucky to...|\n",
      "|B000NKGYMK|A22T74YNRM8NTK|   5.0|    Alaska Sourdough|Make the most sub...|\n",
      "|B000NKGYMK| A7VSVB6Z0JHOV|   2.0|    Alaska Sourdough|\"It's quaint, I'l...|\n",
      "|B000NKGYMK|A3KIUJTLKA3Y2J|   4.0|    Alaska Sourdough|\"We purchased thi...|\n",
      "|B000NKGYMK| A9XNAKYFS2OBN|   5.0|    Alaska Sourdough|got this for my d...|\n",
      "|B000NKGYMK|A2ALLTFYQ2QEA7|   5.0|    Alaska Sourdough|This book is load...|\n",
      "|0195178548|A1MEHQSDHEI882|   5.0|The Oxford Handbo...|Do you want to br...|\n",
      "|0789480662|A2KI2GU89VBXKG|   3.0|Eyewitness Travel...|It's a good book,...|\n",
      "|0789480662|A13HDF4J03LQ81|   4.0|Eyewitness Travel...|\"I bought this bo...|\n",
      "|B0000CJHIO| AUCYV4UOTSU3Y|   5.0|Hunting The Hard Way|Hunting the Hard ...|\n",
      "|B0000CJHIO|A1UHTWM53B5KM1|   4.0|Hunting The Hard Way|\"I have no quarre...|\n",
      "|1562650076|A2B3X891TTT91M|   5.0|Little One, Maid ...|I was introduced ...|\n",
      "+----------+--------------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242687"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_rating = rating_df.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "\n",
    "# Show the sampled data\n",
    "sampled_rating.show()\n",
    "sampled_rating.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af2a5e3-e2f9-41bb-906b-b1b95ddd5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id_threshold = 200\n",
    "user_id_threshold = 10\n",
    "product_id_counts = sampled_rating.groupBy(\"book_id\").count().filter(f\"count >= {product_id_threshold}\")\n",
    "user_id_counts = sampled_rating.groupBy(\"user_id\").count().filter(f\"count >= {user_id_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "536678d5-15c0-4e06-8480-ff30699d7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = sampled_rating.join(product_id_counts, on=\"book_id\", how=\"inner\") \\\n",
    "                .join(user_id_counts, on=\"user_id\", how=\"inner\") \\\n",
    "                .drop(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "249d38f0-b400-4a7d-87fb-8386ec3c4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer_product = StringIndexer(inputCol=\"book_id\", outputCol=\"BookIdIndex\")\n",
    "string_indexer_user = StringIndexer(inputCol=\"user_id\", outputCol=\"UserIdIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "259a5653-36e9-44ca-bafe-f4354115efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[string_indexer_product, string_indexer_user])\n",
    "indexed_df = pipeline.fit(filtered_df).transform(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2818464-a1c9-4353-b4b7-563783e4d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = indexed_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2527229a-7ea9-4beb-a111-261c873874ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"UserIdIndex\", itemCol=\"BookIdIndex\", ratingCol=\"rating\")\n",
    "model = als.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50fba1ef-6e9a-411b-815c-237efcbb4d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): nan\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a1bf9bc-cbfa-4311-a02a-3b108ab6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "user_recs = model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "300d258f-add7-4853-a8c7-d873ce06633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 223.0 failed 1 times, most recent failure: Lost task 3.0 in stage 223.0 (TID 1064) (reem executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\n",
      "\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.PlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.ServerSocket.implAccept(Unknown Source)\n",
      "\tat java.net.ServerSocket.accept(Unknown Source)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\n",
      "\t... 15 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\t... 1 more\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\n",
      "\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.PlainSocketImpl.accept(Unknown Source)\n",
      "\tat java.net.ServerSocket.implAccept(Unknown Source)\n",
      "\tat java.net.ServerSocket.accept(Unknown Source)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\n",
      "\t... 15 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    book_id_to_title = dict(\n",
    "        indexed_df.select(\"BookIdIndex\", \"title\")\n",
    "        .rdd.map(lambda r: (r.BookIdIndex, r.title))\n",
    "        .collect()\n",
    "    )\n",
    "    print(\"Lookup dictionary created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    # Add additional error handling or logging as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97c47f-890e-4219-9db7-9f631861d620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
